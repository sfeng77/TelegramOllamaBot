[1mdiff --git a/bot.py b/bot.py[m
[1mindex 1959394..2a6eb8c 100644[m
[1m--- a/bot.py[m
[1m+++ b/bot.py[m
[36m@@ -1,6 +1,7 @@[m
 import os[m
 import json[m
 import logging[m
[32m+[m[32mimport time[m
 import aiohttp[m
 from dotenv import load_dotenv[m
 from telegram import Update, InlineKeyboardButton, InlineKeyboardMarkup[m
[36m@@ -25,7 +26,8 @@[m [mAVAILABLE_MODELS = {[m
     'llama2': 'llama2',[m
     'llama2-uncensored': 'llama2-uncensored',[m
     'mistral': 'mistral',[m
[31m-    'neural-chat': 'neural-chat'[m
[32m+[m[32m    'neural-chat': 'neural-chat',[m
[32m+[m[32m    'gpt-oss:20b': 'gpt-oss:20b'[m
 }[m
 [m
 # 默认模型[m
[36m@@ -89,8 +91,9 @@[m [masync def button_callback(update: Update, context: ContextTypes.DEFAULT_TYPE) ->[m
         else:[m
             await query.edit_message_text('无效的模型选择')[m
 [m
[31m-async def query_ollama(prompt: str, model: str) -> str:[m
[31m-    """向 Ollama API 发送请求"""[m
[32m+[m[32masync def query_ollama(prompt: str, model: str) -> tuple[str, float]:[m
[32m+[m[32m    """向 Ollama API 发送请求，返回回复和生成时间"""[m
[32m+[m[32m    start_time = time.time()[m
     async with aiohttp.ClientSession() as session:[m
         try:[m
             async with session.post([m
[36m@@ -103,12 +106,19 @@[m [masync def query_ollama(prompt: str, model: str) -> str:[m
             ) as response:[m
                 if response.status == 200:[m
                     result = await response.json()[m
[31m-                    return result.get('response', '抱歉，我现在无法回答这个问题。')[m
[32m+[m[32m                    end_time = time.time()[m
[32m+[m[32m                    generation_time = end_time - start_time[m
[32m+[m[32m                    response_text = result.get('response', '抱歉，我现在无法回答这个问题。')[m
[32m+[m[32m                    return response_text, generation_time[m
                 else:[m
[31m-                    return f"API 请求失败，状态码：{response.status}"[m
[32m+[m[32m                    end_time = time.time()[m
[32m+[m[32m                    generation_time = end_time - start_time[m
[32m+[m[32m                    return f"API 请求失败，状态码：{response.status}", generation_time[m
         except Exception as e:[m
             logger.error(f"请求 Ollama API 时出错: {str(e)}")[m
[31m-            return "抱歉，与 AI 模型通信时出现错误。请确保 Ollama 服务正在运行。"[m
[32m+[m[32m            end_time = time.time()[m
[32m+[m[32m            generation_time = end_time - start_time[m
[32m+[m[32m            return "抱歉，与 AI 模型通信时出现错误。请确保 Ollama 服务正在运行。", generation_time[m
 [m
 async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:[m
     """处理用户消息"""[m
[36m@@ -126,14 +136,23 @@[m [masync def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE) ->[m
     )[m
     [m
     try:[m
[31m-        # 获取 AI 回复[m
[31m-        ai_response = await query_ollama(user_message, current_model)[m
[32m+[m[32m        # 获取 AI 回复和生成时间[m
[32m+[m[32m        ai_response, generation_time = await query_ollama(user_message, current_model)[m
         [m
         # 删除"正在思考"消息[m
         await thinking_message.delete()[m
         [m
[32m+[m[32m        # 格式化生成时间[m
[32m+[m[32m        if generation_time < 1:[m
[32m+[m[32m            time_str = f"{generation_time*1000:.0f}ms"[m
[32m+[m[32m        else:[m
[32m+[m[32m            time_str = f"{generation_time:.2f}s"[m
[32m+[m[41m        [m
[32m+[m[32m        # 在回复中添加生成时间信息[m
[32m+[m[32m        final_response = f"{ai_response}\n\n⏱️ 生成时间：{time_str}"[m
[32m+[m[41m        [m
         # 发送 AI 回复[m
[31m-        await update.message.reply_text(ai_response)[m
[32m+[m[32m        await update.message.reply_text(final_response)[m
     except Exception as e:[m
         logger.error(f"处理消息时出错: {str(e)}")[m
         await thinking_message.edit_text("抱歉，处理您的消息时出现错误。请稍后重试。")[m
