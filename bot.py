import asyncio
import logging
import logging.handlers
import os
import time
from functools import wraps
from pathlib import Path
from typing import Any, Awaitable, Callable

import aiohttp
from dotenv import load_dotenv
from telegram import InlineKeyboardButton, InlineKeyboardMarkup, Update
from telegram.ext import (
    Application,
    CallbackQueryHandler,
    CommandHandler,
    ContextTypes,
    MessageHandler,
    filters,
)

from backends import storage

LOG_DIR = Path("logs")
LOG_DIR.mkdir(parents=True, exist_ok=True)

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.handlers.RotatingFileHandler(
            LOG_DIR / "bot.log",
            maxBytes=10 * 1024 * 1024,
            backupCount=5,
            encoding="utf-8",
        ),
    ],
)

conversation_logger = logging.getLogger("conversation")
conversation_handler = logging.handlers.RotatingFileHandler(
    LOG_DIR / "conversations.log",
    maxBytes=10 * 1024 * 1024,
    backupCount=5,
    encoding="utf-8",
)
conversation_formatter = logging.Formatter("%(asctime)s - %(message)s")
conversation_handler.setFormatter(conversation_formatter)
conversation_logger.addHandler(conversation_handler)
conversation_logger.setLevel(logging.INFO)

logger = logging.getLogger(__name__)

STORAGE_AVAILABLE = False
STORAGE_DB_PATH: str | None = None


def split_long_message(text: str, max_length: int = 4000) -> list[str]:
    """å°†é•¿æ¶ˆæ¯åˆ†å‰²æˆå¤šä¸ªè¾ƒçŸ­çš„æ¶ˆæ¯"""
    if len(text) <= max_length:
        return [text]
    
    messages = []
    current_message = ""
    
    # æŒ‰æ®µè½åˆ†å‰²
    paragraphs = text.split('\n\n')
    
    for paragraph in paragraphs:
        # å¦‚æœå½“å‰æ®µè½åŠ ä¸Šå½“å‰æ¶ˆæ¯è¶…è¿‡é™åˆ¶
        if len(current_message) + len(paragraph) + 2 > max_length:
            if current_message:
                messages.append(current_message.strip())
                current_message = paragraph
            else:
                # å¦‚æœå•ä¸ªæ®µè½å°±è¶…è¿‡é™åˆ¶ï¼ŒæŒ‰å¥å­åˆ†å‰²
                sentences = paragraph.split('. ')
                for sentence in sentences:
                    if len(current_message) + len(sentence) + 2 > max_length:
                        if current_message:
                            messages.append(current_message.strip())
                        current_message = sentence
                    else:
                        current_message += ". " + sentence if current_message else sentence
        else:
            current_message += "\n\n" + paragraph if current_message else paragraph
    
    if current_message:
        messages.append(current_message.strip())
    
    return messages

REASONING_KEYS: set[str] = {"reasoning", "thinking", "thoughts"}


def _stringify_reasoning_segments(obj: Any) -> list[str]:
    """Recursively flatten reasoning data into plain text segments."""
    segments: list[str] = []
    if isinstance(obj, str):
        stripped = obj.strip()
        if stripped:
            segments.append(stripped)
        return segments
    if isinstance(obj, dict):
        type_hint = obj.get("type")
        text_value = obj.get("text")
        if isinstance(text_value, str) and type_hint in {"reasoning", "thinking"}:
            stripped = text_value.strip()
            if stripped:
                segments.append(stripped)
        elif isinstance(text_value, str) and type_hint is None:
            stripped = text_value.strip()
            if stripped:
                segments.append(stripped)
        for key in ("content", "value", "messages", "parts"):
            if key in obj:
                segments.extend(_stringify_reasoning_segments(obj[key]))
        for key, value in obj.items():
            if key in REASONING_KEYS:
                segments.extend(_stringify_reasoning_segments(value))
        return segments
    if isinstance(obj, list):
        if obj and all(isinstance(item, (int, float)) for item in obj):
            return segments
        for item in obj:
            segments.extend(_stringify_reasoning_segments(item))
        return segments
    return segments


def extract_reasoning_text(payload: dict[str, Any]) -> str:
    """Pull any reasoning or thinking content from an Ollama response payload."""
    collected: list[str] = []

    def walk(obj: Any) -> None:
        if isinstance(obj, dict):
            for key, value in obj.items():
                if key in {"context", "embedding"}:
                    continue
                if key in REASONING_KEYS:
                    collected.extend(_stringify_reasoning_segments(value))
                elif isinstance(value, (dict, list)):
                    walk(value)
        elif isinstance(obj, list):
            if obj and all(isinstance(item, (int, float)) for item in obj):
                return
            for item in obj:
                walk(item)

    walk(payload)

    unique_text: list[str] = []
    for item in collected:
        stripped = item.strip()
        if stripped and stripped not in unique_text:
            unique_text.append(stripped)

    return "\n\n".join(unique_text)

async def send_long_message(update: Update, text: str, max_length: int = 4000):
    """å‘é€å¯èƒ½å¾ˆé•¿çš„æ¶ˆæ¯ï¼Œè‡ªåŠ¨åˆ†å‰²"""
    messages = split_long_message(text, max_length)
    
    for i, message in enumerate(messages):
        if i == 0:
            # ç¬¬ä¸€æ¡æ¶ˆæ¯ç›´æ¥å›å¤
            await update.message.reply_text(message)
        else:
            # åç»­æ¶ˆæ¯ä½œä¸ºæ–°æ¶ˆæ¯å‘é€
            await update.message.reply_text(f"ï¼ˆç»­ {i+1}/{len(messages)}ï¼‰\n\n{message}")
        
        # æ·»åŠ çŸ­æš‚å»¶è¿Ÿï¼Œé¿å…å‘é€è¿‡å¿«
        if i < len(messages) - 1:
            await asyncio.sleep(0.5)
    
    # è®°å½•é•¿æ¶ˆæ¯åˆ†å‰²æƒ…å†µ
    if len(messages) > 1:
        logger.info(f"é•¿æ¶ˆæ¯å·²åˆ†å‰²ä¸º {len(messages)} æ¡æ¶ˆæ¯å‘é€")

async def send_ai_response(update: Update, ai_response: str, time_str: str, thinking: str | None = None) -> None:
    """Send the AI response and optional reasoning text to the user."""
    sections: list[str] = []
    if thinking:
        sanitized_thinking = thinking.strip()
        if sanitized_thinking:
            sections.append("Thought process (model reasoning):\n<<BEGIN THOUGHT>>\n" + sanitized_thinking + "\n<<END THOUGHT>>")

    response_body = ai_response.strip()
    if response_body:
        sections.append(response_body)

    sections.append(f"Generated in {time_str}")

    combined_message = "\n\n".join(section for section in sections if section)
    await send_long_message(update, combined_message)

async def on_startup(application: Application) -> None:
    """Initialise persistent storage when the bot starts."""
    global STORAGE_AVAILABLE, STORAGE_DB_PATH

    db_path = os.getenv('DB_PATH') or str(LOG_DIR / "conversations.db")

    try:
        await storage.init_db(db_path)
    except Exception as exc:
        STORAGE_AVAILABLE = False
        STORAGE_DB_PATH = None
        logger.error("Failed to initialise persistent storage at %s: %s", db_path, exc)
    else:
        STORAGE_AVAILABLE = True
        STORAGE_DB_PATH = db_path
        logger.info("Persistent history database initialised at %s", db_path)

# Load environment variables
load_dotenv()
TELEGRAM_BOT_TOKEN = os.getenv('TELEGRAM_BOT_TOKEN')
OLLAMA_API_URL = "http://localhost:11434/api/generate"

ALLOWED_USER_IDS_ENV = os.getenv('ALLOWED_TELEGRAM_USER_IDS', '')
ALLOWED_USER_IDS: set[int] = set()
if ALLOWED_USER_IDS_ENV:
    for raw_value in ALLOWED_USER_IDS_ENV.split(','):
        trimmed_value = raw_value.strip()
        if not trimmed_value:
            continue
        try:
            ALLOWED_USER_IDS.add(int(trimmed_value))
        except ValueError:
            logger.warning("Invalid user ID '%s' in ALLOWED_TELEGRAM_USER_IDS; ignoring.", trimmed_value)
if not ALLOWED_USER_IDS:
    logger.warning("ALLOWED_TELEGRAM_USER_IDS is empty; bot will deny all users until configured.")

PERSONAS_DIR = Path('personas')


def load_personas(directory: Path) -> dict[str, str]:
    personas: dict[str, str] = {}
    if not directory.exists():
        return personas
    for persona_file in sorted(directory.glob('*.txt')):
        try:
            content = persona_file.read_text(encoding='utf-8').strip()
        except UnicodeDecodeError:
            logging.getLogger(__name__).warning(
                "Unable to read persona file %s due to encoding error",
                persona_file
            )
            continue
        if content:
            personas[persona_file.stem] = content
    return personas


PERSONAS = load_personas(PERSONAS_DIR)
DEFAULT_PERSONA_KEY = 'ChatBuddy' if 'ChatBuddy' in PERSONAS else next(iter(PERSONAS), None)
if not PERSONAS:
    logger.warning("No persona definitions found in %s", PERSONAS_DIR)

# å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨
AVAILABLE_MODELS = {
    # 'deepseek_small': 'deepseek-r1:1.5b',
    # 'deepseek_large': 'deepseek-r1:32b',
    # 'llama2': 'llama2',
    # 'llama2-uncensored': 'llama2-uncensored',
    # 'mistral': 'mistral',
    # 'neural-chat': 'neural-chat',
    'gpt-oss:20b': 'gpt-oss:20b'
}

# é»˜è®¤æ¨¡å‹
DEFAULT_MODEL = 'gpt-oss:20b'

RECAP_RANGES: dict[str, tuple[int, str]] = {
    "hour": (3600, "Last Hour"),
    "day": (86400, "Last Day"),
}

RECAP_ALIAS_MAP: dict[str, str] = {
    "1h": "hour",
    "h": "hour",
    "24h": "day",
    "d": "day",
    "last hour": "hour",
    "last day": "day",
}


def resolve_recap_timespan(selection: str) -> tuple[int, str] | None:
    """Normalize user input into a supported recap timespan definition."""
    normalized = selection.lower().strip()
    normalized = RECAP_ALIAS_MAP.get(normalized, normalized)
    return RECAP_RANGES.get(normalized)

async def _deny_access(update: Update) -> None:
    """Inform the user that access is restricted."""
    if update.callback_query:
        await update.callback_query.answer('Access denied. This bot is limited to approved users.', show_alert=True)
        return
    if update.effective_message:
        await update.effective_message.reply_text('Access denied. This bot is limited to approved users.')


async def ensure_user_allowed(update: Update) -> bool:
    """Return True when the update comes from a whitelisted user."""
    user = update.effective_user
    if not user:
        logger.warning('Received update without an effective user; denying access.')
        await _deny_access(update)
        return False

    user_identifier = f"{user.username or 'Unknown'}({user.id})"
    if user.id in ALLOWED_USER_IDS:
        return True

    logger.warning('Unauthorized access attempt from %s', user_identifier)
    conversation_logger.info(f"[Unauthorized {user_identifier}] Access denied by whitelist policy")
    await _deny_access(update)
    return False


def whitelist_required(handler: Callable[..., Awaitable[Any]]):
    """Wrap a handler so it only runs for whitelisted users."""
    @wraps(handler)
    async def wrapper(update: Update, context: ContextTypes.DEFAULT_TYPE, *args: Any, **kwargs: Any):
        if not await ensure_user_allowed(update):
            return
        return await handler(update, context, *args, **kwargs)

    return wrapper

@whitelist_required
async def start(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """å¤„ç† /start å‘½ä»¤"""
    user_id = update.effective_user.id
    username = update.effective_user.username or "Unknown"

    state = context.chat_data

    if not state.get('model'):
        state['model'] = DEFAULT_MODEL

    if not state.get('conversation_history'):
        state['conversation_history'] = []

    if PERSONAS and not state.get('persona'):
        state['persona'] = DEFAULT_PERSONA_KEY

    logger.info(f"ç”¨æˆ· {username}({user_id}) æ‰§è¡Œ/startå‘½ä»¤")
    conversation_logger.info(f"[ç”¨æˆ· {username}({user_id})] æ‰§è¡Œ/startå‘½ä»¤ - åˆå§‹åŒ–å¯¹è¯")

    current_model_key = state['model']
    current_model = AVAILABLE_MODELS[current_model_key]
    persona_value = state.get('persona') if PERSONAS else None
    if not persona_value:
        persona_value = "None"

    welcome_message = (
        "æ¬¢è¿ä½¿ç”¨ AI èŠå¤©æœºå™¨äººï¼\n\n"
        "è¿™ä¸ªæœºå™¨äººä½¿ç”¨ Ollama æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚\n"
        f"å½“å‰ä½¿ç”¨çš„æ¨¡å‹æ˜¯ï¼š{current_model}\n"
        f"å½“å‰çš„åŠ©æ‰‹è§’è‰²æ˜¯ï¼š{persona_value}\n"
        "/help å‘½ä»¤å¯æŸ¥çœ‹ä½¿ç”¨è¯´æ˜ã€‚"
    )
    await send_long_message(update, welcome_message)

@whitelist_required
async def help_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """å¤„ç† /help å‘½ä»¤"""
    help_text = """
ä½¿ç”¨è¯´æ˜ï¼š

1. ç›´æ¥å‘é€æ–‡å­—æ¶ˆæ¯å³å¯ä¸ AI å¯¹è¯
2. æœºå™¨äººä¼šè®°ä½å¯¹è¯ä¸Šä¸‹æ–‡ï¼Œæä¾›æ›´è¿è´¯çš„å¯¹è¯ä½“éªŒ
3. ä½¿ç”¨ /model å‘½ä»¤å¯ä»¥åˆ‡æ¢ä¸åŒçš„ AI æ¨¡å‹
4. ä½¿ç”¨ /forget å‘½ä»¤å¯ä»¥æ¸…é™¤å¯¹è¯å†å²ï¼Œé‡æ–°å¼€å§‹
5. ä½¿ç”¨ /persona å‘½ä»¤å¯ä»¥é€‰æ‹©åŠ©æ‰‹è§’è‰²
6. ä½¿ç”¨ /thoughts å‘½ä»¤å¯ä»¥å¼€å…³æ€è€ƒè¿‡ç¨‹å±•ç¤º
7. å½“å‰æ”¯æŒçš„æ¨¡å‹ï¼š
{}
8. å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·å°è¯•é‡æ–°å‘é€æ¶ˆæ¯
""".format('\n'.join(f'   - {name}' for name in AVAILABLE_MODELS.keys()))
    await send_long_message(update, help_text)

@whitelist_required
async def thoughts_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handle /thoughts command to toggle reasoning visibility."""
    if not update.message:
        return

    user_id = update.effective_user.id
    username = update.effective_user.username or "Unknown"
    state = context.chat_data
    state.setdefault('show_thoughts', False)
    current_state = state['show_thoughts']

    new_state = current_state
    if context.args:
        choice = context.args[0].lower()
        if choice in {"on", "enable", "true", "1"}:
            new_state = True
        elif choice in {"off", "disable", "false", "0"}:
            new_state = False
        else:
            await update.message.reply_text("Usage: /thoughts [on|off]")
            return
    else:
        new_state = not current_state

    state['show_thoughts'] = new_state
    state_label = "enabled" if new_state else "disabled"
    logger.info(f"Thought display {state_label} for user {username}({user_id})")

    if new_state:
        await update.message.reply_text("Thought process display enabled. I'll include sections between <<BEGIN THOUGHT>> and <<END THOUGHT>>.")
    else:
        await update.message.reply_text("Thought process display disabled. I'll keep the reasoning hidden.")


@whitelist_required
async def model_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """å¤„ç† /model å‘½ä»¤"""
    user_id = update.effective_user.id
    username = update.effective_user.username or "Unknown"
    state = context.chat_data
    current_model_key = state.get('model', DEFAULT_MODEL)

    logger.info(f"ç”¨æˆ· {username}({user_id}) æ‰§è¡Œ/modelå‘½ä»¤ - å½“å‰æ¨¡å‹: {current_model_key}")

    keyboard = []
    for name, model in AVAILABLE_MODELS.items():
        current = 'âœ“ ' if state.get('model', DEFAULT_MODEL) == name else ''
        keyboard.append([InlineKeyboardButton(f"{current}{model}", callback_data=f"model_{name}")])

    reply_markup = InlineKeyboardMarkup(keyboard)
    await update.message.reply_text('è¯·é€‰æ‹©è¦ä½¿ç”¨çš„ AI æ¨¡å‹ï¼š', reply_markup=reply_markup)

@whitelist_required
async def persona_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """Handle /persona command."""
    if not PERSONAS:
        await update.message.reply_text("No personas are configured for this bot.")
        return

    state = context.chat_data
    if not state.get('persona'):
        state['persona'] = DEFAULT_PERSONA_KEY

    keyboard: list[list[InlineKeyboardButton]] = []
    current_persona = state.get('persona')
    for name in PERSONAS:
        prefix = "[*] " if current_persona == name else ""
        keyboard.append([InlineKeyboardButton(f"{prefix}{name}", callback_data=f"persona_{name}")])

    reply_markup = InlineKeyboardMarkup(keyboard)
    await update.message.reply_text("Select the assistant persona:", reply_markup=reply_markup)

@whitelist_required
async def forget_command(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """å¤„ç† /forget å‘½ä»¤"""
    user_id = update.effective_user.id
    username = update.effective_user.username or "Unknown"

    state = context.chat_data
    history_length = len(state.get('conversation_history', []))

    state['conversation_history'] = []

    logger.info(f"ç”¨æˆ· {username}({user_id}) æ‰§è¡Œ/forgetå‘½ä»¤ - æ¸…é™¤äº†{history_length}è½®å¯¹è¯å†å²")
    conversation_logger.info(f"[ç”¨æˆ· {username}({user_id})] æ‰§è¡Œ/forgetå‘½ä»¤ - æ¸…é™¤äº†{history_length}è½®å¯¹è¯å†å²")

    await update.message.reply_text("ğŸ§¹ å·²æ¸…é™¤æ‰€æœ‰å¯¹è¯å†å²ï¼Œæˆ‘ä»¬å¯ä»¥é‡æ–°å¼€å§‹å¯¹è¯äº†ï¼")

@whitelist_required
async def button_callback(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """å¤„ç†æŒ‰é’®å›è°ƒ"""
    query = update.callback_query
    user_id = update.effective_user.id
    username = update.effective_user.username or "Unknown"
    state = context.chat_data

    await query.answer()

    if query.data.startswith('model_'):
        model_name = query.data[6:]  # ç§»é™¤ 'model_' å‰ç¼€
        if model_name in AVAILABLE_MODELS:
            old_model = state.get('model', DEFAULT_MODEL)
            state['model'] = model_name
            new_model = AVAILABLE_MODELS[model_name]

            logger.info(f"ç”¨æˆ· {username}({user_id}) åˆ‡æ¢æ¨¡å‹: {old_model} -> {model_name}")
            conversation_logger.info(f"[ç”¨æˆ· {username}({user_id})] åˆ‡æ¢æ¨¡å‹: {old_model} -> {new_model}")

            await query.edit_message_text(f'å·²åˆ‡æ¢åˆ°æ¨¡å‹ï¼š{new_model}')
        else:
            logger.warning(f"ç”¨æˆ· {username}({user_id}) å°è¯•é€‰æ‹©æ— æ•ˆæ¨¡å‹: {model_name}")
            await query.edit_message_text('æ— æ•ˆçš„æ¨¡å‹é€‰æ‹©')

    elif query.data.startswith('persona_'):
        persona_name = query.data[8:]
        if persona_name in PERSONAS:
            old_persona = state.get('persona', DEFAULT_PERSONA_KEY)
            state['persona'] = persona_name
            state['conversation_history'] = []

            logger.info(f"User {username}({user_id}) switched persona: {old_persona} -> {persona_name}")
            conversation_logger.info(
                f"[User {username}({user_id})] switched persona: {old_persona} -> {persona_name}"
            )

            await query.edit_message_text(
                f'Persona updated to {persona_name}. Conversation history cleared.'
            )
        else:
            logger.warning(f"User {username}({user_id}) tried invalid persona: {persona_name}")
            await query.edit_message_text('Invalid persona selection')


async def query_ollama(
    prompt: str,
    model: str,
    context_history: list | None = None,
    persona_prompt: str = "",
    speaker_label: str | None = None,
) -> tuple[str, str | None, float]:
    """Call the Ollama API and return the generated text, reasoning text, and elapsed time."""
    start_time = time.time()

    history_count = len(context_history) if context_history else 0
    persona_label = "default"
    if persona_prompt:
        first_line = persona_prompt.strip().splitlines()[0]
        persona_label = first_line[:60]

    sections: list[str] = []

    if persona_prompt:
        sections.append("System instructions:\n" + persona_prompt.strip())

    if history_count:
        context_lines: list[str] = []
        for item in context_history[-10:]:
            user_line = item.get("user", "")
            assistant_line = item.get("assistant", "")
            context_lines.append(f"User: {user_line}")
            context_lines.append(f"Assistant: {assistant_line}")
        sections.append("Conversation history:\n" + "\n".join(context_lines))

    current_prompt = speaker_label or prompt
    sections.append(f"User: {current_prompt}\nAssistant:")
    full_prompt = "\n\n".join(sections)

    logger.info(
        "Calling Ollama - model: %s, history: %s, persona: %s",
        model,
        history_count,
        persona_label,
    )

    logger.info(
        "Full prompt:\n %s",
        full_prompt,
    )


    async with aiohttp.ClientSession() as session:
        try:
            async with session.post(
                OLLAMA_API_URL,
                json={
                    "model": model,
                    "prompt": full_prompt,
                    "stream": False,
                },
            ) as response:
                if response.status == 200:
                    result: dict[str, Any] = await response.json()
                    logger.debug("Ollama response: %s", result)
                    end_time = time.time()
                    generation_time = end_time - start_time
                    response_text = result.get("response") or "The model returned no content."
                    thinking_text = extract_reasoning_text(result) or None
                    if thinking_text:
                        logger.debug("Ollama reasoning captured (%s chars)", len(thinking_text))
                    logger.info(
                        "Ollama response ready - model: %s, time: %.2fs, length: %s",
                        model,
                        generation_time,
                        len(response_text),
                    )
                    return response_text, thinking_text, generation_time

                end_time = time.time()
                generation_time = end_time - start_time
                error_msg = f"Ollama returned status {response.status}"
                logger.error(
                    "Ollama error - model: %s, status: %s, time: %.2fs",
                    model,
                    response.status,
                    generation_time,
                )
                return error_msg, None, generation_time
        except Exception as exc:
            logger.error("Error calling Ollama: %s", exc)
            end_time = time.time()
            generation_time = end_time - start_time
            error_msg = "Unable to reach the Ollama service right now."
            logger.error(
                "Ollama exception - model: %s, error: %s, time: %.2fs",
                model,
                exc,
                generation_time,
            )
            return error_msg, None, generation_time

@whitelist_required
async def handle_message(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """å¤„ç†ç”¨æˆ·æ¶ˆæ¯"""
    if not update.message or not update.message.text:
        if update.message:
            await update.message.reply_text("æŠ±æ­‰ï¼Œæˆ‘åªå¤„ç†æ–‡æœ¬æ¶ˆæ¯ã€‚è¯·å‘é€æ–‡å­—å†…å®¹ã€‚")
        return

    user_message = update.message.text
    user_id = update.effective_user.id
    username = update.effective_user.username or "Unknown"
    state = context.chat_data

    if not state.get('model'):
        state['model'] = DEFAULT_MODEL

    if not state.get('conversation_history'):
        state['conversation_history'] = []

    if PERSONAS:
        if not state.get('persona'):
            state['persona'] = DEFAULT_PERSONA_KEY
        current_persona = state.get('persona')
        persona_prompt = PERSONAS.get(current_persona, '')
    else:
        current_persona = None
        persona_prompt = ''

    state.setdefault('show_thoughts', False)
    show_thoughts = state['show_thoughts']

    current_model_key = state['model']
    current_model = AVAILABLE_MODELS[current_model_key]
    conversation_history = state['conversation_history']

    chat = update.effective_chat
    chat_id = chat.id if chat else None
    chat_title = chat.title if chat and chat.title else None
    if chat_title:
        chat_label = f"{chat_title}({chat_id})"
    elif chat_id is not None:
        chat_label = f"Chat {chat_id}"
    else:
        chat_label = "Chat unknown"

    is_group_chat = bool(chat and chat.type in {'group', 'supergroup'})
    speaker_name = update.effective_user.username or update.effective_user.full_name or "Unknown"
    speaker_label = f"@{speaker_name} ({user_id})"
    user_entry_text = f"{speaker_label}: {user_message}" if is_group_chat else user_message

    conversation_round = len(conversation_history) + 1

    logger.info(f"ç”¨æˆ· {username}({user_id}) åœ¨ {chat_label} å‘é€æ¶ˆæ¯ - ç¬¬{conversation_round}è½®å¯¹è¯")
    conversation_logger.info(f"[{chat_label}] [ç”¨æˆ· {username}({user_id})] ç¬¬{conversation_round}è½® - ç”¨æˆ·: {user_entry_text}")

    persona_line = f"\nPersona: {current_persona}" if current_persona else ""
    thinking_message = await update.message.reply_text(
        f"Thinking...\nUsing model: {current_model}{persona_line}"
    )

    try:
        ai_response, thinking_text, generation_time = await query_ollama(
            user_message,
            current_model,
            conversation_history,
            persona_prompt,
            user_entry_text,
        )

        await thinking_message.delete()

        logger.info(f"AIå›å¤ç”Ÿæˆå®Œæˆ - ç”¨æˆ·: {username}({user_id}), æ¨¡å‹: {current_model}, æ—¶é—´: {generation_time:.2f}s")
        conversation_logger.info(f"[{chat_label}] [ç”¨æˆ· {username}({user_id})] ç¬¬{conversation_round}è½® - AIå›å¤: {ai_response}")
        if thinking_text:
            conversation_logger.info(f"[{chat_label}] [ç”¨æˆ· {username}({user_id})] ç¬¬{conversation_round}è½® - AIæ€è€ƒ: {thinking_text}")

        conversation_history.append({
            'user': user_entry_text,
            'assistant': ai_response
        })

        if len(conversation_history) > 20:
            del conversation_history[:-20]
            logger.info(f"{chat_label} çš„å¯¹è¯å†å²å·²é™åˆ¶ä¸º20è½®")

        if generation_time < 1:
            time_str = f"{generation_time*1000:.0f}ms"
        else:
            time_str = f"{generation_time:.2f}s"

        display_thinking = thinking_text if show_thoughts else None
        await send_ai_response(update, ai_response, time_str, display_thinking)

        conversation_logger.info(f"[{chat_label}] [ç”¨æˆ· {username}({user_id})] ç¬¬{conversation_round}è½®å¯¹è¯å®Œæˆ - å†å²è®°å½•é•¿åº¦: {len(conversation_history)}")

    except Exception as e:
        logger.error(f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {str(e)}")
        conversation_logger.error(f"[{chat_label}] [ç”¨æˆ· {username}({user_id})] ç¬¬{conversation_round}è½®å¯¹è¯å‡ºé”™: {str(e)}")
        await thinking_message.edit_text("æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æ¶ˆæ¯æ—¶å‡ºç°é”™è¯¯ã€‚è¯·ç¨åé‡è¯•ã€‚")

async def error_handler(update: Update, context: ContextTypes.DEFAULT_TYPE) -> None:
    """é”™è¯¯å¤„ç†å™¨"""
    logger.error(f"å¤„ç†æ›´æ–°æ—¶å‡ºé”™: {context.error}")
    if update and update.effective_message:
        try:
            await update.effective_message.reply_text("æŠ±æ­‰ï¼Œå¤„ç†æ‚¨çš„æ¶ˆæ¯æ—¶å‡ºç°é”™è¯¯ã€‚è¯·ç¨åé‡è¯•ã€‚")
        except Exception as e:
            logger.error(f"å‘é€é”™è¯¯æ¶ˆæ¯å¤±è´¥: {str(e)}")

def main() -> None:
    """ä¸»å‡½æ•°"""
    if not TELEGRAM_BOT_TOKEN:
        logger.error("æœªè®¾ç½® TELEGRAM_BOT_TOKEN ç¯å¢ƒå˜é‡")
        return

    # åˆ›å»ºåº”ç”¨
    application = Application.builder().token(TELEGRAM_BOT_TOKEN).build()

    # æ·»åŠ é”™è¯¯å¤„ç†å™¨
    application.add_error_handler(error_handler)

    # æ·»åŠ å¤„ç†å™¨
    application.add_handler(CommandHandler("start", start))
    application.add_handler(CommandHandler("help", help_command))
    application.add_handler(CommandHandler("thoughts", thoughts_command))
    application.add_handler(CommandHandler("model", model_command))
    application.add_handler(CommandHandler("persona", persona_command))
    application.add_handler(CommandHandler("forget", forget_command))
    application.add_handler(CallbackQueryHandler(button_callback))
    application.add_handler(MessageHandler(filters.TEXT & ~filters.COMMAND, handle_message))

    # å¯åŠ¨æœºå™¨äºº
    application.run_polling(allowed_updates=Update.ALL_TYPES, drop_pending_updates=True)

if __name__ == '__main__':
    main() 
